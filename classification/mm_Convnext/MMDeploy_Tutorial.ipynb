{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e5ea362-c5e4-4ae8-8933-ca5693e82ba5",
   "metadata": {},
   "source": [
    "# Export onnx with your trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ab70f74-7d61-4c3f-8db4-32c55c4b75f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint: /home/test/carasml/classification/mm_Convnext/mmpretrain/work_dirs/convnext-v2-tiny_32xb32_in1k-384px_custom/epoch_10.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def get_latest_checkpoint(base_dir):\n",
    "    ckpt_file = os.path.join(base_dir, \"last_checkpoint\")\n",
    "    if os.path.exists(ckpt_file):\n",
    "        with open(ckpt_file, \"r\") as f:\n",
    "            relative_ckpt_path = f.readline().strip()\n",
    "            full_ckpt_path = os.path.join(base_dir, relative_ckpt_path)\n",
    "            if os.path.exists(full_ckpt_path):\n",
    "                return full_ckpt_path\n",
    "    return None\n",
    "\n",
    "# the logs stored in work_dirs/convnext-v2-tiny_32xb32_in1k-384px_custom/\n",
    "latest_ckpt = get_latest_checkpoint(\"mmpretrain/work_dirs/convnext-v2-tiny_32xb32_in1k-384px_custom/\")\n",
    "print(f\"Using checkpoint: {latest_ckpt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ba48f75-8943-44f5-89f5-6d21f4c157b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# revise the code according to your model\n",
    "# the second line is the location of exporting script, base on your nature of your task (classification/ detection/ segmentation)\n",
    "# the third line is the location of configs of your model\n",
    "# the fourth line is the location of weights (pth) of your model\n",
    "# the fifth line is a sample of image\n",
    "# the sixth line is exporting location\n",
    "# the seventh line is choosing model to run on cpu or cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e010393a-c28e-4eb7-8d58-a178ef7daa5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/09 13:27:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Start pipeline mmdeploy.apis.pytorch2onnx.torch2onnx in subprocess\n",
      "07/09 13:27:50 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmpretrain\" in the \"Codebases\" registry tree. As a workaround, the current \"Codebases\" registry in \"mmdeploy\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmpretrain\" is a correct scope, or whether the registry is initialized.\n",
      "07/09 13:27:50 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmpretrain\" in the \"mmpretrain_tasks\" registry tree. As a workaround, the current \"mmpretrain_tasks\" registry in \"mmdeploy\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmpretrain\" is a correct scope, or whether the registry is initialized.\n",
      "07/09 13:27:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.\n",
      "Loads checkpoint by local backend from path: /home/test/carasml/classification/mm_Convnext/mmpretrain/work_dirs/convnext-v2-tiny_32xb32_in1k-384px_custom/epoch_10.pth\n",
      "07/09 13:27:50 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - DeprecationWarning: get_onnx_config will be deprecated in the future. \n",
      "07/09 13:27:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Export PyTorch model to ONNX: mmdeploy_model/convnext/end2end.onnx.\n",
      "07/09 13:27:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Execute onnx optimize passes.\n",
      "07/09 13:27:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Finish pipeline mmdeploy.apis.pytorch2onnx.torch2onnx\n",
      "07/09 13:27:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Start pipeline mmdeploy.apis.utils.utils.to_backend in main process\n",
      "07/09 13:27:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Finish pipeline mmdeploy.apis.utils.utils.to_backend\n",
      "07/09 13:27:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - visualize onnxruntime model start.\n",
      "07/09 13:27:53 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmpretrain\" in the \"Codebases\" registry tree. As a workaround, the current \"Codebases\" registry in \"mmdeploy\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmpretrain\" is a correct scope, or whether the registry is initialized.\n",
      "07/09 13:27:53 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmpretrain\" in the \"mmpretrain_tasks\" registry tree. As a workaround, the current \"mmpretrain_tasks\" registry in \"mmdeploy\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmpretrain\" is a correct scope, or whether the registry is initialized.\n",
      "07/09 13:27:53 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmpretrain\" in the \"backend_classifiers\" registry tree. As a workaround, the current \"backend_classifiers\" registry in \"mmdeploy\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmpretrain\" is a correct scope, or whether the registry is initialized.\n",
      "07/09 13:27:53 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - The library of onnxruntime custom ops doesnot exist: \n",
      "07/09 13:27:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - visualize onnxruntime model success.\n",
      "07/09 13:27:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - visualize pytorch model start.\n",
      "07/09 13:27:56 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmpretrain\" in the \"Codebases\" registry tree. As a workaround, the current \"Codebases\" registry in \"mmdeploy\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmpretrain\" is a correct scope, or whether the registry is initialized.\n",
      "07/09 13:27:56 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmpretrain\" in the \"mmpretrain_tasks\" registry tree. As a workaround, the current \"mmpretrain_tasks\" registry in \"mmdeploy\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmpretrain\" is a correct scope, or whether the registry is initialized.\n",
      "07/09 13:27:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.\n",
      "Loads checkpoint by local backend from path: /home/test/carasml/classification/mm_Convnext/mmpretrain/work_dirs/convnext-v2-tiny_32xb32_in1k-384px_custom/epoch_10.pth\n",
      "07/09 13:27:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - visualize pytorch model success.\n",
      "07/09 13:27:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - All process success.\n"
     ]
    }
   ],
   "source": [
    "%run mmdeploy/tools/deploy.py \\\n",
    "mmdeploy/configs/mmpretrain/classification_onnxruntime_dynamic.py \\\n",
    "mmpretrain/work_dirs/convnext-v2-tiny_32xb32_in1k-384px_custom/convnext-v2-tiny_32xb32_in1k-384px_custom.py \\\n",
    "{latest_ckpt} \\\n",
    "mmpretrain/data/ZhangLabData/CellData/OCT/test/NORMAL/NORMAL-1569-1.jpeg \\\n",
    "--work-dir mmdeploy_model/convnext \\\n",
    "--device cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9466eb9a-480b-418c-a6a9-e1dc9f981a58",
   "metadata": {},
   "source": [
    "# Inference the onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8529d8b-1331-43be-9dbf-2a8941f5328c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "# providers set provider priority cuda or cpu\n",
    "import onnxruntime\n",
    "sess = onnxruntime.InferenceSession(\"mmdeploy_model/convnext/end2end.onnx\", providers=[\"CUDAExecutionProvider\"])\n",
    "print(sess.get_providers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f85e59a5-2b5d-49f5-a92f-1a7bae3c0a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<DataSample(\n",
      "\n",
      "META INFORMATION\n",
      "    img_path: mmpretrain/data/ZhangLabData/CellData/OCT/test/NORMAL/NORMAL-1569-1.jpeg\n",
      "    ori_shape: (496, 1024)\n",
      "    num_classes: 4\n",
      "    img_shape: (384, 384)\n",
      "    scale_factor: (0.375, 0.7741935483870968)\n",
      "\n",
      "DATA FIELDS\n",
      "    pred_score: tensor([0.0791, 0.0510, 0.0577, 0.8122])\n",
      "    pred_label: tensor([3])\n",
      "\n",
      ") at 0x7410b06c3040>]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from mmdeploy.apis import inference_model\n",
    "is the official method to run an onnx, but it require torch\n",
    "'''\n",
    "#Classification\n",
    "from mmdeploy.apis import inference_model\n",
    "result = inference_model(\n",
    "    model_cfg='mmpretrain/work_dirs/convnext-v2-tiny_32xb32_in1k-384px_custom/convnext-v2-tiny_32xb32_in1k-384px_custom.py',\n",
    "    deploy_cfg='mmdeploy/configs/mmpretrain/classification_onnxruntime_dynamic.py',\n",
    "    backend_files=['mmdeploy_model/convnext/end2end.onnx'],\n",
    "    img='mmpretrain/data/ZhangLabData/CellData/OCT/test/NORMAL/NORMAL-1569-1.jpeg',\n",
    "    device='cpu')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1067aaa2-7c6e-4433-84a7-e8496a6df5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available providers: ['CPUExecutionProvider']\n",
      "Model output: [[0.07913636 0.05096158 0.0577339  0.81216806]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "this is an unofficial method to run an onnx that require no torch\n",
    "however, you need to check the 'model_cfg' to understand how the image was preprocessed\n",
    "which include normalization method and resize and padding method\n",
    "you may search keywords \"data_preprocessor\" and \"mean\" and \"std\" for normalization \n",
    "and \"pipeline\" and \"scale\" for resize and padding method\n",
    "'''\n",
    "#Classification \n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# we resize\n",
    "def resize(image, target_size):\n",
    "    # Resize the image with aspect ratio preserved\n",
    "    image = image.resize((target_size, target_size), Image.BICUBIC)\n",
    "\n",
    "    return image\n",
    "    \n",
    "# Load the ONNX model with cpu or cuda execution provider\n",
    "sess = onnxruntime.InferenceSession(\"mmdeploy_model/convnext/end2end.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "print(\"Available providers:\", sess.get_providers())\n",
    "\n",
    "# Define the image path\n",
    "image_path = \"mmpretrain/data/ZhangLabData/CellData/OCT/test/NORMAL/NORMAL-1569-1.jpeg\"\n",
    "\n",
    "# Load and preprocess the image\n",
    "image = Image.open(image_path)\n",
    "image = resize(image, target_size=384)\n",
    "\n",
    "# Convert the input tensor to a numpy array (ONNXRuntime uses numpy arrays)\n",
    "input_array = np.array(image)\n",
    "input_array = np.expand_dims(input_array,axis=0)\n",
    "input_array = np.expand_dims(input_array,axis=0)\n",
    "\n",
    "'''\n",
    "according to config\n",
    "    data_preprocessor = dict(\n",
    "        mean=[\n",
    "            123.675,\n",
    "            116.28,\n",
    "            103.53,\n",
    "        ],\n",
    "        num_classes=4,\n",
    "        std=[\n",
    "            58.395,\n",
    "            57.12,\n",
    "            57.375,\n",
    "        ],\n",
    "        to_rgb=True)\n",
    "'''\n",
    "#normalize image\n",
    "mean = np.array([123.675, 116.28, 103.53]).reshape(1, 3, 1, 1) \n",
    "std = np.array([58.395, 57.12, 57.375]).reshape(1, 3, 1, 1) \n",
    "input_array = (input_array - mean) / std\n",
    "input_array = input_array.astype('float32')\n",
    "\n",
    "# Get the model's input name (usually 'input' or something similar)\n",
    "input_name = sess.get_inputs()[0].name\n",
    "\n",
    "# Run inference on the input image\n",
    "outputs = sess.run(None, {input_name: input_array})\n",
    "\n",
    "# Get the output (typically the class probabilities)\n",
    "output = outputs[0]\n",
    "\n",
    "# Print the output\n",
    "print(\"Model output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1690474a-be08-431e-a2c1-1b8465a796aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mmconvnext)",
   "language": "python",
   "name": "mmconvnext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
