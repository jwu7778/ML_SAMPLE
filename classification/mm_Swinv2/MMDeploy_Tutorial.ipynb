{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e5ea362-c5e4-4ae8-8933-ca5693e82ba5",
   "metadata": {},
   "source": [
    "# Export onnx with your trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ba48f75-8943-44f5-89f5-6d21f4c157b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# revise the code according to your model\n",
    "# the second line is the location of exporting script, base on your nature of your task (classification/ detection/ segmentation)\n",
    "# the third line is the location of configs of your model\n",
    "# the fourth line is the location of weights (pth) of your model\n",
    "# the fifth line is a sample of image\n",
    "# the sixth line is exporting location\n",
    "# the seventh line is choosing model to run on cpu or cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "468d3982-90ac-499c-b49d-e5aee67b4b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "def get_latest_checkpoint(base_dir=\"mmpretrain/work_dirs\"):\n",
    "    subdirs = [os.path.join(base_dir, d) for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "    subdirs.sort(key=os.path.getmtime, reverse=True)  # Sort by modification time, newest first\n",
    "\n",
    "    for d in subdirs:\n",
    "        ckpt_file = os.path.join(d, \"last_checkpoint\")\n",
    "        if os.path.exists(ckpt_file):\n",
    "            with open(ckpt_file, \"r\") as f:\n",
    "                relative_ckpt_path = f.readline().strip()\n",
    "                full_ckpt_path = os.path.join(d, relative_ckpt_path)\n",
    "                if os.path.exists(full_ckpt_path):\n",
    "                    return full_ckpt_path\n",
    "    return None\n",
    "\n",
    "latest_ckpt = get_latest_checkpoint()\n",
    "print(latest_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e010393a-c28e-4eb7-8d58-a178ef7daa5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/29 17:30:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Start pipeline mmdeploy.apis.pytorch2onnx.torch2onnx in subprocess\n",
      "05/29 17:30:52 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmpretrain\" in the \"Codebases\" registry tree. As a workaround, the current \"Codebases\" registry in \"mmdeploy\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmpretrain\" is a correct scope, or whether the registry is initialized.\n",
      "05/29 17:30:52 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmpretrain\" in the \"mmpretrain_tasks\" registry tree. As a workaround, the current \"mmpretrain_tasks\" registry in \"mmdeploy\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmpretrain\" is a correct scope, or whether the registry is initialized.\n",
      "05/29 17:30:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.\n",
      "Loads checkpoint by local backend from path: /home/z890/Downloads/ml_sample/classification/mm_Swinv2/mmpretrain/work_dirs/swinv2-tiny-w8_16xb64_in1k-256px_place/epoch_5.pth\n",
      "05/29 17:30:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Delete `relative_position_index` and `relative_coords_table` since we always re-init these params according to the `window_size`, which might cause unwanted but unworried warnings when loading checkpoint.\n",
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "missing keys in source state_dict: backbone.stages.0.blocks.0.attn.w_msa.relative_coords_table, backbone.stages.0.blocks.0.attn.w_msa.relative_position_index, backbone.stages.0.blocks.1.attn.w_msa.relative_coords_table, backbone.stages.0.blocks.1.attn.w_msa.relative_position_index, backbone.stages.1.blocks.0.attn.w_msa.relative_coords_table, backbone.stages.1.blocks.0.attn.w_msa.relative_position_index, backbone.stages.1.blocks.1.attn.w_msa.relative_coords_table, backbone.stages.1.blocks.1.attn.w_msa.relative_position_index, backbone.stages.2.blocks.0.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.0.attn.w_msa.relative_position_index, backbone.stages.2.blocks.1.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.1.attn.w_msa.relative_position_index, backbone.stages.2.blocks.2.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.2.attn.w_msa.relative_position_index, backbone.stages.2.blocks.3.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.3.attn.w_msa.relative_position_index, backbone.stages.2.blocks.4.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.4.attn.w_msa.relative_position_index, backbone.stages.2.blocks.5.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.5.attn.w_msa.relative_position_index, backbone.stages.3.blocks.0.attn.w_msa.relative_coords_table, backbone.stages.3.blocks.0.attn.w_msa.relative_position_index, backbone.stages.3.blocks.1.attn.w_msa.relative_coords_table, backbone.stages.3.blocks.1.attn.w_msa.relative_position_index\n",
      "\n",
      "05/29 17:30:53 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - DeprecationWarning: get_onnx_config will be deprecated in the future. \n",
      "05/29 17:30:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Export PyTorch model to ONNX: mmdeploy_model/swinv2/end2end.onnx.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z890/Downloads/ml_sample/classification/mm_Swinv2/mmcv/mmcv/cnn/bricks/transformer.py:125: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  output_h = math.ceil(input_h / stride_h)\n",
      "/home/z890/Downloads/ml_sample/classification/mm_Swinv2/mmcv/mmcv/cnn/bricks/transformer.py:126: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  output_w = math.ceil(input_w / stride_w)\n",
      "/home/z890/Downloads/ml_sample/classification/mm_Swinv2/mmcv/mmcv/cnn/bricks/transformer.py:127: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  pad_h = max((output_h - 1) * stride_h +\n",
      "/home/z890/Downloads/ml_sample/classification/mm_Swinv2/mmcv/mmcv/cnn/bricks/transformer.py:129: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  pad_w = max((output_w - 1) * stride_w +\n",
      "/home/z890/Downloads/ml_sample/classification/mm_Swinv2/mmcv/mmcv/cnn/bricks/transformer.py:143: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if pad_h > 0 or pad_w > 0:\n",
      "/home/z890/Downloads/ml_sample/classification/mm_Swinv2/mmdeploy/mmdeploy/codebase/mmpretrain/models/utils/attention.py:68: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert L == H * W, f\"The query length {L} doesn't match the input \"\\\n",
      "/home/z890/Downloads/ml_sample/classification/mm_Swinv2/mmdeploy/mmdeploy/codebase/mmpretrain/models/utils/attention.py:75: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min(H, W) == window_size:\n",
      "/home/z890/Downloads/ml_sample/classification/mm_Swinv2/mmdeploy/mmdeploy/codebase/mmpretrain/models/utils/attention.py:80: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  elif min(H, W) < window_size:\n",
      "/home/z890/Downloads/ml_sample/classification/mm_Swinv2/mmpretrain/mmpretrain/models/utils/attention.py:462: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
      "/home/z890/Downloads/ml_sample/classification/mm_Swinv2/mmdeploy/mmdeploy/codebase/mmpretrain/models/utils/attention.py:132: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if H != H_pad or W != W_pad:\n",
      "/home/z890/Downloads/ml_sample/classification/mm_Swinv2/mmpretrain/mmpretrain/models/utils/embed.py:392: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert L == H * W, 'input feature has wrong size'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/29 17:30:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Execute onnx optimize passes.\n",
      "05/29 17:30:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Finish pipeline mmdeploy.apis.pytorch2onnx.torch2onnx\n",
      "05/29 17:30:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Start pipeline mmdeploy.apis.utils.utils.to_backend in main process\n",
      "05/29 17:30:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Finish pipeline mmdeploy.apis.utils.utils.to_backend\n",
      "05/29 17:30:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - visualize onnxruntime model start.\n",
      "05/29 17:30:57 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmpretrain\" in the \"Codebases\" registry tree. As a workaround, the current \"Codebases\" registry in \"mmdeploy\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmpretrain\" is a correct scope, or whether the registry is initialized.\n",
      "05/29 17:30:57 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmpretrain\" in the \"mmpretrain_tasks\" registry tree. As a workaround, the current \"mmpretrain_tasks\" registry in \"mmdeploy\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmpretrain\" is a correct scope, or whether the registry is initialized.\n",
      "05/29 17:30:57 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmpretrain\" in the \"backend_classifiers\" registry tree. As a workaround, the current \"backend_classifiers\" registry in \"mmdeploy\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmpretrain\" is a correct scope, or whether the registry is initialized.\n",
      "05/29 17:30:57 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - The library of onnxruntime custom ops doesnot exist: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-05-29 17:30:57.726813864 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-29 17:30:57.726830459 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/29 17:30:57 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - expect input device cuda but get cuda:0.\n",
      "05/29 17:31:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - visualize onnxruntime model success.\n",
      "05/29 17:31:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - visualize pytorch model start.\n",
      "05/29 17:31:33 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmpretrain\" in the \"Codebases\" registry tree. As a workaround, the current \"Codebases\" registry in \"mmdeploy\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmpretrain\" is a correct scope, or whether the registry is initialized.\n",
      "05/29 17:31:33 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmpretrain\" in the \"mmpretrain_tasks\" registry tree. As a workaround, the current \"mmpretrain_tasks\" registry in \"mmdeploy\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmpretrain\" is a correct scope, or whether the registry is initialized.\n",
      "05/29 17:31:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.\n",
      "Loads checkpoint by local backend from path: /home/z890/Downloads/ml_sample/classification/mm_Swinv2/mmpretrain/work_dirs/swinv2-tiny-w8_16xb64_in1k-256px_place/epoch_5.pth\n",
      "05/29 17:31:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Delete `relative_position_index` and `relative_coords_table` since we always re-init these params according to the `window_size`, which might cause unwanted but unworried warnings when loading checkpoint.\n",
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "missing keys in source state_dict: backbone.stages.0.blocks.0.attn.w_msa.relative_coords_table, backbone.stages.0.blocks.0.attn.w_msa.relative_position_index, backbone.stages.0.blocks.1.attn.w_msa.relative_coords_table, backbone.stages.0.blocks.1.attn.w_msa.relative_position_index, backbone.stages.1.blocks.0.attn.w_msa.relative_coords_table, backbone.stages.1.blocks.0.attn.w_msa.relative_position_index, backbone.stages.1.blocks.1.attn.w_msa.relative_coords_table, backbone.stages.1.blocks.1.attn.w_msa.relative_position_index, backbone.stages.2.blocks.0.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.0.attn.w_msa.relative_position_index, backbone.stages.2.blocks.1.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.1.attn.w_msa.relative_position_index, backbone.stages.2.blocks.2.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.2.attn.w_msa.relative_position_index, backbone.stages.2.blocks.3.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.3.attn.w_msa.relative_position_index, backbone.stages.2.blocks.4.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.4.attn.w_msa.relative_position_index, backbone.stages.2.blocks.5.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.5.attn.w_msa.relative_position_index, backbone.stages.3.blocks.0.attn.w_msa.relative_coords_table, backbone.stages.3.blocks.0.attn.w_msa.relative_position_index, backbone.stages.3.blocks.1.attn.w_msa.relative_coords_table, backbone.stages.3.blocks.1.attn.w_msa.relative_position_index\n",
      "\n",
      "05/29 17:31:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - visualize pytorch model success.\n",
      "05/29 17:31:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - All process success.\n"
     ]
    }
   ],
   "source": [
    "%run mmdeploy/tools/deploy.py \\\n",
    "mmdeploy/configs/mmpretrain/classification_onnxruntime_dynamic.py \\\n",
    "mmpretrain/work_dirs/swinv2-tiny-w8_16xb64_in1k-256px_place/swinv2-tiny-w8_16xb64_in1k-256px_place.py \\\n",
    "{latest_ckpt} \\\n",
    "mmpretrain/data/places365/val/ballroom/Places365_val_00001732.jpg \\\n",
    "--work-dir mmdeploy_model/swinv2 \\\n",
    "--device cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9466eb9a-480b-418c-a6a9-e1dc9f981a58",
   "metadata": {},
   "source": [
    "# Inference the onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8529d8b-1331-43be-9dbf-2a8941f5328c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-05-29 17:31:45.247775561 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-29 17:31:45.247789852 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# providers set provider priority cuda or cpu\n",
    "import onnxruntime\n",
    "sess = onnxruntime.InferenceSession(\"mmdeploy_model/swinv2/end2end.onnx\", providers=[\"CUDAExecutionProvider\"])\n",
    "print(sess.get_providers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f85e59a5-2b5d-49f5-a92f-1a7bae3c0a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/29 17:31:52 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmpretrain\" in the \"Codebases\" registry tree. As a workaround, the current \"Codebases\" registry in \"mmdeploy\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmpretrain\" is a correct scope, or whether the registry is initialized.\n",
      "05/29 17:31:52 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmpretrain\" in the \"mmpretrain_tasks\" registry tree. As a workaround, the current \"mmpretrain_tasks\" registry in \"mmdeploy\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmpretrain\" is a correct scope, or whether the registry is initialized.\n",
      "05/29 17:31:52 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmpretrain\" in the \"backend_classifiers\" registry tree. As a workaround, the current \"backend_classifiers\" registry in \"mmdeploy\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmpretrain\" is a correct scope, or whether the registry is initialized.\n",
      "05/29 17:31:52 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - The library of onnxruntime custom ops doesnot exist: \n",
      "05/29 17:31:52 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - expect input device cuda but get cuda:0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-05-29 17:31:52.298685018 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-29 17:31:52.298698078 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<DataSample(\n",
      "\n",
      "META INFORMATION\n",
      "    img_shape: (256, 256)\n",
      "    ori_shape: (256, 256)\n",
      "    img_path: mmpretrain/data/places365/val/ballroom/Places365_val_00001732.jpg\n",
      "    scale_factor: (1.140625, 1.140625)\n",
      "    num_classes: 28\n",
      "\n",
      "DATA FIELDS\n",
      "    pred_score: tensor([0.0058, 0.0167, 0.0689, 0.0767, 0.0246, 0.0134, 0.0311, 0.0079, 0.0382,\n",
      "        0.1331, 0.0139, 0.0109, 0.0169, 0.0136, 0.0576, 0.0059, 0.0091, 0.0617,\n",
      "        0.0495, 0.0139, 0.0088, 0.0251, 0.0286, 0.0405, 0.0296, 0.0816, 0.0765,\n",
      "        0.0397])\n",
      "    pred_label: tensor([9])\n",
      "\n",
      ") at 0x7d58483f4f40>]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from mmdeploy.apis import inference_model\n",
    "is the official method to run an onnx, but it require torch\n",
    "'''\n",
    "#Classification\n",
    "from mmdeploy.apis import inference_model\n",
    "result = inference_model(\n",
    "    model_cfg='mmpretrain/work_dirs/swinv2-tiny-w8_16xb64_in1k-256px_place/swinv2-tiny-w8_16xb64_in1k-256px_place.py',\n",
    "    deploy_cfg='mmdeploy/configs/mmpretrain/classification_onnxruntime_dynamic.py',\n",
    "    backend_files=['mmdeploy_model/swinv2/end2end.onnx'],\n",
    "    img='mmpretrain/data/places365/val/ballroom/Places365_val_00001732.jpg',\n",
    "    device='cuda')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1067aaa2-7c6e-4433-84a7-e8496a6df5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available providers: ['CPUExecutionProvider']\n",
      "Model output: [[0.00576688 0.01667846 0.06892081 0.07669645 0.02459264 0.0134371\n",
      "  0.03105985 0.00792458 0.03824529 0.1331148  0.0139368  0.0109138\n",
      "  0.01692919 0.01361539 0.05759621 0.00593901 0.0091106  0.0616653\n",
      "  0.04947931 0.01392222 0.00882916 0.02504956 0.02863899 0.0404804\n",
      "  0.02963363 0.08163732 0.07646955 0.03971672]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "this is an unofficial method to run an onnx that require no torch\n",
    "however, you need to check the 'model_cfg' to understand how the image was preprocessed\n",
    "which include normalization method and resize and padding method\n",
    "you may search keywords \"data_preprocessor\" and \"mean\" and \"std\" for normalization \n",
    "and \"pipeline\" and \"scale\" for resize and padding method\n",
    "'''\n",
    "#Classification \n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def resize_edge(img, scale=292, edge='short', interpolation='bicubic'):\n",
    "    \"\"\"Resize so that the short edge is `scale`, maintaining aspect ratio.\"\"\"\n",
    "    width, height = img.size\n",
    "    if edge == 'short':\n",
    "        if width < height:\n",
    "            new_width = scale\n",
    "            new_height = int(scale * height / width)\n",
    "        else:\n",
    "            new_height = scale\n",
    "            new_width = int(scale * width / height)\n",
    "    elif edge == 'long':\n",
    "        if width > height:\n",
    "            new_width = scale\n",
    "            new_height = int(scale * height / width)\n",
    "        else:\n",
    "            new_height = scale\n",
    "            new_width = int(scale * width / height)\n",
    "    else:\n",
    "        raise ValueError(\"edge must be 'short' or 'long'\")\n",
    "\n",
    "    resample = Image.BICUBIC if interpolation == 'bicubic' else Image.BILINEAR\n",
    "    return img.resize((new_width, new_height), resample)\n",
    "\n",
    "def center_crop(img, crop_size=256):\n",
    "    \"\"\"Crop the center `crop_size` x `crop_size` region from the image.\"\"\"\n",
    "    width, height = img.size\n",
    "    left = (width - crop_size) // 2\n",
    "    top = (height - crop_size) // 2\n",
    "    right = left + crop_size\n",
    "    bottom = top + crop_size\n",
    "    return img.crop((left, top, right, bottom))\n",
    "\n",
    "    \n",
    "# Load the ONNX model with CUDA execution provider\n",
    "sess = onnxruntime.InferenceSession(\"mmdeploy_model/swinv2/end2end.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "print(\"Available providers:\", sess.get_providers())\n",
    "\n",
    "# Define the image path\n",
    "image_path = \"mmpretrain/data/places365/val/ballroom/Places365_val_00001732.jpg\"\n",
    "\n",
    "# Load and preprocess the image\n",
    "image = Image.open(image_path)\n",
    "'''\n",
    "according to config\n",
    "    test_pipeline = [\n",
    "        dict(type='LoadImageFromFile'),\n",
    "        dict(\n",
    "            backend='pillow',\n",
    "            edge='short',\n",
    "            interpolation='bicubic',\n",
    "            scale=292,\n",
    "            type='ResizeEdge'),\n",
    "        dict(crop_size=256, type='CenterCrop'),\n",
    "        dict(type='PackInputs'),\n",
    "    ]\n",
    "'''\n",
    "image = resize_edge(image, scale=292, edge='short', interpolation='bicubic')\n",
    "image = center_crop(image, crop_size=256)\n",
    "\n",
    "# Convert the input tensor to a numpy array (ONNXRuntime uses numpy arrays)\n",
    "input_array = np.transpose(image, (2,0,1))\n",
    "input_array = np.expand_dims(input_array,axis=0)\n",
    "\n",
    "'''\n",
    "according to config\n",
    "    data_preprocessor = dict(\n",
    "        mean=[\n",
    "            123.675,\n",
    "            116.28,\n",
    "            103.53,\n",
    "        ],\n",
    "        num_classes=28,\n",
    "        std=[\n",
    "            58.395,\n",
    "            57.12,\n",
    "            57.375,\n",
    "        ],\n",
    "        to_rgb=True)\n",
    "'''\n",
    "#normalize image\n",
    "mean = np.array([123.675, 116.28, 103.53]).reshape(1, 3, 1, 1) \n",
    "std = np.array([58.395, 57.12, 57.375]).reshape(1, 3, 1, 1) \n",
    "input_array = (input_array - mean) / std\n",
    "input_array = input_array.astype('float32')\n",
    "\n",
    "# Get the model's input name (usually 'input' or something similar)\n",
    "input_name = sess.get_inputs()[0].name\n",
    "\n",
    "# Run inference on the input image\n",
    "outputs = sess.run(None, {input_name: input_array})\n",
    "\n",
    "# Get the output (typically the class probabilities)\n",
    "output = outputs[0]\n",
    "\n",
    "# Print the output\n",
    "print(\"Model output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1690474a-be08-431e-a2c1-1b8465a796aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cmms)",
   "language": "python",
   "name": "cmms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
